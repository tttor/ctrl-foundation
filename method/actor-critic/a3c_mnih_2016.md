# Asynchronous Methods for Deep Reinforcement Learning
* 33rd International Conference on Machine Learning, New York, NY, USA, 2016;
* Volodymyr Mnih, Adrià Puigdomènech Badia, Mehdi Mirza, Alex Graves, Timothy P. Lillicrap, Tim Harley, 
  David Silver, Koray Kavukcuoglu
* https://arxiv.org/abs/1602.01783
* http://proceedings.mlr.press/v48/mniha16.html
* https://www.youtube.com/watch?v=Ajjc08-iPx8&feature=youtu.be (for robots, goto: 0:44)

## problem
* drawbacks of Deep RL algorithms based on experience replay  
  * it uses more memory and computation per real interaction; and 
  * it requires off-policy learning algorithms that can update from data generated by an older policy.

## observation
* multiple actors-learners **running in parallel** are likely to be **exploring different parts** of the environment.
  * use different exploration policies in each actor-learner to maximize this diversity
  
## idea: A3C
* framework for deep reinforcement learning that **uses asynchronous gradient descent** for
  optimization of deep neural network controllers.
* asynchronously execute multiple agents in parallel (Instead of experience replay) on multiple instances of the environment. 
  * This parallelism **decorrelates** the agents’ data into a more stationary process, since 
    at any given time-step the parallel agents will be experiencing a variety of different states.
* use multiple CPU threads on a single machine (instead of using separate machines and a parameter server)
  * removes the communication costs of sending gradients and parameters over multiple machine, cf Gorila framework
  * enables us to use Hogwild! (Recht et al., 2011) style updates for training
* running different exploration policies in different threads, 
  * the overall changes being made to the parameters are likely to be less correlated in time 
    than a single agent applying online updates, hence
    * do not use a replay memory and 
    * rely on parallel actors employing different exploration policies to perform the stabilizing role 
      undertaken by experience replay in the DQN training algorithm. 
  * obtain a reduction in training time that is roughly linear in the number of parallel actor-learners
  * able to use on-policy reinforcement learning methods such as Sarsa and actor-critic to 
    train neural networks in a stable way (since we no longer rely on experience replay for stabilizing learning)
* 4 multi-threaded asynchronous variants
  * Asynchronous one-step Q-learning
    * Each thread interacts with its own copy of the environment and 
      at each step computes a gradient of the Q-learning loss.
    * accumulate gradients over multiple timesteps before they are applied
      * reduces the chances of multiple actor learners overwriting each other’s updates. 
      * provides some ability to trade off computational efficiency for data efficiency
    * giving each thread a different exploration policy helps improve robustness  
  * Asynchronous one-step Sarsa
    * same as asynchronous one-step Q-learning except that it uses a different target value for Q(s, a).
  * Asynchronous n-step Q-learning
    * operates in the forward view by explicitly computing n-step returns
  * Asynchronous advantage actor-critic (A3C)
    * maintains a policy `\pi(a_t|s_t; \theta)`  and an estimate of the value function `V(s_t; \theta_v)` 
    * operates in the **forward view** and uses the same mix of **n-step returns** to 
      update both the policy and the value-function.
    * while the parameters `\theta` of the policy and `\theta_v` of the value function are 
      shown as being separate for generality, always **share some of the parameters** in practice    
    * typically use a convolutional neural network that has (with all non-output layers shared)
      * one softmax output for the policy `\pi(a_t|s_t; \theta)` and 
      * one linear output for the value function `V(s_t; \theta_v)` 
    * adding the entropy of the policy to the objective function to improve exploration by 
      discouraging premature convergence to suboptimal deterministic policies

## setup
* task: 
  * Atari domain (via ALE), 
    * int the discrete action domain, the action output is a Softmax
  * TORCS 3D car racing simulator
  * Continuous Action Control Using the MuJoCo: 
    * a set of rigid body physics domains with contact dynamics:
      pole swing-up, quadruped locomotion, planar biped walking, balancing, 2D/3D target reaching
    * input for nets: 
      * the physical state: joint positions and velocities, the target position
      * RGB pixel inputs
    * two outputs of the policy network are two real number vectors which we treat as 
      the mean vector `$\mu$` and scalar variance `$\sigma^2$`of a multidimensional normal distribution with 
      a spherical covariance. 
      * To act, the input is passed through the model to the output layer where 
        we sample from the normal distribution determined by `$\mu$` and `$\sigma^2$`    
     * In our experiments with continuous control problems, 
       the networks for policy network and value network do **not share** any parameters
       Finally, since the episodes were typically at most several hundred time steps long,
     * did not use any bootstrapping in the policy or value function updates and 
       batched each episode into a single update.
  * exploring 3D mazes purely from visual inputs via Deepmind's Labyrinth
* 3 different optimization algorithms in our asynchronous framework
  * SGD with momentum, 
  * RMSProp (Tieleman & Hinton, 2012) without shared statistics, and 
  * RMSProp with shared statistics.

## result
* The best performing method, **an asynchronous variant of actor-critic**, surpasses
  the current state-of-the-art on the Atari domain while training for
  half the time on **a single multi-core CPU** instead of a GPU.
* using parallel actorlearners to update a shared model had
  a stabilizing effect on the learning process of the three value-based methods
* asynchronous one-step Q-learning and Sarsa algorithms exhibit superlinear speedups that 
  cannot be explained by purely computational gains. 
  * one-step methods (one-step Q and one-step Sarsa) often require less data to 
    achieve a particular score when using more parallel actor-learners;
    due to positive effect of multiple threads to reduce the bias in one-step methods. 
* our proposed framework 
  * scales well with the number of parallel workers, making efficient use of resources.
  * stable and do not collapse or diverge once they are learning
  * make stable training of neural networks through reinforcement learning possible with 
    both value- based and policy-based methods, off-policy as well as on- policy methods, and 
    in discrete as well as continuous domains
  
## misc
* unstability of deepNN on RL caused by
  * the sequence of observed data encountered by an online RL agent is non-stationary, and 
  * online RL updates are strongly correlated.
* approaches to stabilize deep neural networks on online RL algorithms:
  * by storing the agent’s data in an experience replay memory, 
    the data can be batched (Riedmiller, 2005; Schulman et al., 2015a) or 
    randomly sampled (Mnih et al., 2013; 2015; Van Hasselt et al., 2015) from different time-steps. 
  * Aggregating over memory in this way reduces non-stationarity and decorrelates updates, but 
    at the same time limits the methods to off-policy reinforcement learning algorithms.
* One-step Q-learning
  * updates the action value `$Q(s, a)$` toward the one-step return `$r + \lambda max_{a'} Q(s', a'; \theta)$`. 
  * One drawback:
    * that obtaining a reward r only directly affects the value of the state action pair s, a that led to the reward. 
    * The values of other state action pairs are affected only indirectly through the updated value Q(s, a). 
    * make the learning process slow since many updates are required to propagate a reward to 
      the relevant preceding states and actions.
  * One way of propagating rewards faster is by using **n-step returns**
    * a single reward r directly affecting the values of n preceding state action pairs.
  * Advantage fn: `$A_{\pi}(s,a) = Q_{\pi}(s,a) - V_{\pi}(s)$`
    
 ## comment
 * promising way to use multi-threading in planning where we do simulate multiple env+agent interactions,
   note, from Figure S4: the training time is in the order of hours, meaning **no way** for online planning, 
   but what if for offline planning? 
 * plots and tables (on the paper, not the appendix) with analysis are all from Atari game setup, 
   although there are experiments in/with Mujoco/robots (shown in the appendix)
 * in the [demo vid on robots, 0:44](https://www.youtube.com/watch?v=Ajjc08-iPx8&feature=youtu.be), initial state is **fixed**, what if for any initial state?
 
